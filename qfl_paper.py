# -*- coding: utf-8 -*-
"""QFL_paper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u_xzWfbdpPrmww_BCtdOiKoIietnA7Ps

The main code implementing the QFL framework
"""

#Change directory to where the datasets are saved
from google.colab import drive
drive.mount('/content/gdrive')
root_path = '/content/gdrive/My Drive/QFL Paper/'

"""### Setup

Install TensorFlow Federated
"""

# This specific version of TFF is the one compatible with TFQ, the most recent one have some compatibility issues
!pip install tensorflow-federated==0.17.0

"""Install TensorFlow """

#Install this specific version of TF for compatibility reasons
!pip install tensorflow==2.3.1

"""Install TensorFlow Quantum:"""

#Install this version of TFQ for compatibility reasons
!pip install tensorflow-quantum==0.4.0

import nest_asyncio
nest_asyncio.apply()
#The next library is necessary to import the federated data
import h5py
import collections
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff
import tensorflow_quantum as tfq
import cirq
import sympy
tf.compat.v1.enable_v2_behavior()
np.random.seed(0)

"""### load the federated data"""

#load the federated dataset
Test = tff.simulation.hdf5_client_data.HDF5ClientData(root_path+'30clients.hdf5')

#Specify the number of training clients
NumClients = 25
#Shuffle the client IDs
shuffled_ids = Test.client_ids.copy()
tf.random.shuffle(shuffled_ids)
#split the clients into training and testing subgroups 
train_ids = shuffled_ids[0:NumClients]
test_ids = shuffled_ids[NumClients:]
#split the federated datasets into testing and trainging
federated_train_data = [Test.create_tf_dataset_for_client(x) for x in train_ids]
federated_test_data = [Test.create_tf_dataset_for_client(x) for x in test_ids]
#Check the lenght of each dataset, and a sample datapoint
print(len(federated_train_data), federated_train_data[0])
print(len(federated_test_data), federated_test_data[0])

"""### Define the quantum circuit layers

define the layers needed to build the quantum circuit.
Start by creating the quantum cluster state, then the necessary layers for the QCNN.
This portion of the code is based on the [QCNN Tutorial](https://www.tensorflow.org/quantum/tutorials/qcnn)

#### Create the Quantum Cluster state
"""

#Return the quantum circuit of a cluster state applied on the qubits in `bits`
def cluster_state_circuit(bits):
    circuit = cirq.Circuit()
    circuit.append(cirq.H.on_each(bits))
    for this_bit, next_bit in zip(bits, bits[1:] + [bits[0]]):
        circuit.append(cirq.CZ(this_bit, next_bit))
    return circuit

"""#### Define the required unitaries"""

# Cirq quantum circuit to create an arbitrary sinle-qubit unitary
def one_qubit_unitary(bit, symbols):
    return cirq.Circuit(
        cirq.X(bit)**symbols[0],
        cirq.Y(bit)**symbols[1],
        cirq.Z(bit)**symbols[2])

# Cirq quantum circuit to create an arbitrary two-qubit unitary
def two_qubit_unitary(bits, symbols):
    circuit = cirq.Circuit()
    circuit += one_qubit_unitary(bits[0], symbols[0:3])
    circuit += one_qubit_unitary(bits[1], symbols[3:6])
    circuit += [cirq.ZZ(*bits)**symbols[6]]
    circuit += [cirq.YY(*bits)**symbols[7]]
    circuit += [cirq.XX(*bits)**symbols[8]]
    circuit += one_qubit_unitary(bits[0], symbols[9:12])
    circuit += one_qubit_unitary(bits[1], symbols[12:])
    return circuit

# Cirq quantum circuit to perform a parametrized pooling operation.
# This operation reduces entanglement down from two-qubits, to a single qubit.
def two_qubit_pool(source_qubit, sink_qubit, symbols):
    pool_circuit = cirq.Circuit()
    sink_basis_selector = one_qubit_unitary(sink_qubit, symbols[0:3])
    source_basis_selector = one_qubit_unitary(source_qubit, symbols[3:6])
    pool_circuit.append(sink_basis_selector)
    pool_circuit.append(source_basis_selector)
    pool_circuit.append(cirq.CNOT(control=source_qubit, target=sink_qubit))
    pool_circuit.append(sink_basis_selector**-1)
    return pool_circuit

"""#### Define the Quantum convolution layer """

# A cascade application of the two-qubit unitary to all pairs of qubits in 'bits' 
def quantum_conv_circuit(bits, symbols):
    circuit = cirq.Circuit()
    for first, second in zip(bits[0::2], bits[1::2]):
        circuit += two_qubit_unitary([first, second], symbols)
    for first, second in zip(bits[1::2], bits[2::2] + [bits[0]]):
        circuit += two_qubit_unitary([first, second], symbols)
    return circuit

"""#### Define the Quantum pooling layer"""

# A circuit that learns to pool the relevant information from two qubits onto 1
def quantum_pool_circuit(source_bits, sink_bits, symbols):
    circuit = cirq.Circuit()
    for source, sink in zip(source_bits, sink_bits):
        circuit += two_qubit_pool(source, sink, symbols)
    return circuit

"""### Define the QCNN model"""

# The model consists of a sequence of convolution and pooling layers that 
# gradually shrink over time
def create_model_circuit(qubits):
    model_circuit = cirq.Circuit()
    symbols = sympy.symbols('qconv0:63') # 64 learnable circuit parameters
    # Cirq uses sympy.Symbols to map learnable variables. 
    #TensorFlow Quantum scans incoming circuits and replaces these with TensorFlow variables.
    model_circuit += quantum_conv_circuit(qubits, symbols[0:15])
    model_circuit += quantum_pool_circuit(qubits[:4], qubits[4:],
                                          symbols[15:21])
    model_circuit += quantum_conv_circuit(qubits[4:], symbols[21:36])
    model_circuit += quantum_pool_circuit(qubits[4:6], qubits[6:],
                                          symbols[36:42])
    model_circuit += quantum_conv_circuit(qubits[6:], symbols[42:57])
    model_circuit += quantum_pool_circuit([qubits[6]], [qubits[7]],
                                          symbols[57:63])
    return model_circuit

# Create a Keras model to prepare for learning
def create_keras_model():
  cluster_state_bits = cirq.GridQubit.rect(1, 8)
  readout_operators = cirq.Z(cluster_state_bits[-1])

  excitation_input = tf.keras.Input(shape=(), dtype=tf.dtypes.string)
  cluster_state = tfq.layers.AddCircuit()(
      excitation_input, prepend=cluster_state_circuit(cluster_state_bits))

  quantum_model = tfq.layers.PQC(create_model_circuit(cluster_state_bits),
                                readout_operators)(cluster_state)
  return tf.keras.Model(inputs=[excitation_input], outputs=[quantum_model])

"""### Define the TFF Model
Wrap the model in an instance of the `tff.learning.Model` interface in order to train it with TFF. This part builds upon the [TFF tutorial](https://colab.research.google.com/drive/1kCSSFUCU_rxW7MElwZXENe50_VTnvGH1?usp=sharing). 
"""

#TFF Model
def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model = keras_model,
      input_spec= federated_train_data[0].element_spec, 
      loss=tf.keras.losses.MeanSquaredError(), # MSE loss function adopted for learning
      metrics = [tf.keras.metrics.BinaryAccuracy()]) # The accuracy metric used

"""### Build the TFF Iterative Process

In order to start learning in the federated setup, build a federated averaging iterative process, initialize it, then start iterating and evaluate the performance.
"""

# Build the iterative process
#Note that another algorithm rather than federated averaging could have been adopted (e.g., federated SGD,...)
iterative_process = tff.learning.build_federated_averaging_process(
    model_fn=model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.02), # The client's optimizer and learning rate are varied throughout the simulations, we used (Adam, SGD, and RMSprop), see Section 4.2.3 in the paper.
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))

str(iterative_process.initialize.type_signature)

# Initialize the iterative process
state = iterative_process.initialize()

# Start learning (first epoch)
state, metrics = iterative_process.next(state, federated_train_data)
print('round  1, metrics={}'.format(metrics))

"""### Evaluate the performance"""

# Create an evaluation process to see how the model performs
evaluation = tff.learning.build_federated_evaluation(model_fn)

# Check training accuracy
train_metrics = evaluation(state.model, federated_train_data)

str(train_metrics)

# Check testing accuracy
test_metrics = evaluation(state.model, federated_test_data)

str(test_metrics)

"""Either repeat the above process (iterative_process.next) and evaluate the performance in each epoch, or train over multiple epochs as shown below, then evaluate the performance."""

# Perform training for NUM_ROUNDS times
NUM_ROUNDS = 13 # change this value to the desired number of training epochs
for round_num in range(2, NUM_ROUNDS):
  state, metrics = iterative_process.next(state, federated_train_data)
  print('round {:2d}, metrics={}'.format(round_num, metrics))

evaluation = tff.learning.build_federated_evaluation(model_fn)

train_metrics = evaluation(state.model, federated_train_data)

str(train_metrics)

test_metrics = evaluation(state.model, federated_test_data)

str(test_metrics)